###############################
####### ADD USER, ADD GROUP, CONFIGURE HOSTS, CONFIGURE SSH
###############################
- name: Add Group "{{group_name}}" group
  group:
    name: "{{group_name}}"
    state: present
    system: no

- name: Create user "{{user_name}}"
  user:
    name: "{{user_name}}"
    group: "{{group_name}}"
    password: "{{user_password}}"
    state: present

- name: Add admin group to sudo
  lineinfile: "dest=/etc/sudoers regexp='^%{{ group_name }}} ALL' line='%{{ group_name }} ALL=(ALL) NOPASSWD: ALL' state=present"

- name: Build hosts file
  lineinfile:
    dest: /etc/hosts
    regexp: '.*{{ item }}$'
    line: "{{ hostvars[item]['ansible_eth1']['ipv4']['address'] }}   {{ hostvars[item]['ansible_hostname'] }}"
    state: present
  with_items: 
    - '{{ groups["hdfsnamenodes"] }}'
    - '{{ groups["hdfsdatanodes"] }}'
    - '{{ groups["hdfsjournalnodes"] }}'
    - '{{ groups["hdfszookeepers"] }}'

- name: Generate ssh key for "{{ user_name }}"
  user:
    name: "{{ user_name }}"
    generate_ssh_key: yes
    ssh_key_bits: 2048
    ssh_key_file: "/home/{{user_name}}/.ssh/id_rsa"

- name: Create authorized_keys
  file:
    path: /home/{{ user_name }}/.ssh/authorized_keys
    owner: "{{ user_name }}"
    group: "{{ user_name }}"
    state: touch
    mode: 0644

- name: Create authorized_keys
  file:
    path: /home/{{ user_name }}/.ssh/known_hosts
    owner: "{{ user_name }}"
    group: "{{ user_name }}"
    state: touch
    mode: 0644

- name: Add generated ssh key to authorized_keys for {{ user_name }}
  shell: cat /home/{{ user_name }}/.ssh/id_rsa.pub >> /home/{{ user_name }}/.ssh/authorized_keys
  when: inventory_hostname in groups['hdfsnamenodes']

- name: Add host to known_hosts
  shell: ssh-keyscan -H 127.0.0.1 >> /home/{{ user_name }}/.ssh/known_hosts && ssh-keyscan -H localhost >> /home/{{ user_name }}/.ssh/known_hosts && ssh-keyscan -H 0.0.0.0 >> /home/{{ user_name }}/.ssh/known_hosts
  when: inventory_hostname in groups['hdfsnamenodes']

- name: fetch all public ssh keys
  shell: cat /home/{{ user_name }}/.ssh/id_rsa.pub
  register: ssh_keys
  when: inventory_hostname in groups['hdfsnamenodes']

- name: Deploy keys on all servers
  authorized_key:
    user: "{{user_name}}"
    key: "{{item[0]}}"
  delegate_to: "{{item[1]}}"
  with_nested:
    - "{{ssh_keys.stdout}}"
    - "{{groups['hdfsdatanodes']}} + {{groups['hdfsjournalnodes']}} + {{groups['hdfszookeepers']}}"
  when: inventory_hostname in groups['hdfsnamenodes']

- name: Accept fingerprint
  shell: ssh-keyscan -H {{item}} >> /home/{{ user_name }}/.ssh/known_hosts
  become_user: "{{ user_name }}"
  with_items: "{{groups['hdfsnamenodes']}} + {{groups['hdfsdatanodes']}} + {{groups['hdfsjournalnodes']}} + {{groups['hdfszookeepers']}}"  

###############################
####### CREATE DIRECTORIES FOR HADOOP
###############################
- name: Create hdfs folder
  file:
    path: "{{hdfs_folder}}"
    state: directory
    owner: "{{ user_name }}"
    group: "{{ group_name }}"
    mode: 0775
    recurse: yes
  when: inventory_hostname in groups['hdfsnamenodes'] + groups['hdfsdatanodes'] + groups['hdfsjournalnodes']

- name: Create datanode folder
  file:
    path: "{{datanode_directory}}"
    state: directory
    owner: "{{ user_name }}"
    group: "{{ group_name }}"
    mode: 0775
    recurse: yes
  when: inventory_hostname in groups['hdfsdatanodes']

- name: Create namenode folder
  file:
    path: "{{namenode_directory}}"
    state: directory
    owner: "{{ user_name }}"
    group: "{{ group_name }}"
    mode: 0775
    recurse: yes
  when: inventory_hostname in groups['hdfsnamenodes']

- name: Create journalnode folder
  file:
    path: "{{journalnode_directory}}"
    state: directory
    owner: "{{ user_name }}"
    group: "{{ group_name }}"
    mode: 0775
    recurse: yes
  when: inventory_hostname in groups['hdfsjournalnodes']

###############################
####### INSTALL NECCESARY PACKAGES
###############################
- name: Install packages
  package:
    name: "{{ item }}"
    state: latest
  with_items:
    - wget
    - rsync

- name: Ensure folder {{role_path}}/files/ exists
  local_action: file path="{{role_path}}/files" state=directory
  run_once: yes
  become: no

- name: Check if Java Oracle is downloaded
  local_action: stat path={{role_path}}/files/{{ java_rpm_name }}
  run_once: yes
  become: no
  register: java_oracle_file

- name: Download Java Oracle locally
  local_action: "shell wget -O {{role_path}}/files/{{ java_rpm_name }} --no-cookies --no-check-certificate --header 'Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie' 'http://download.oracle.com/otn-pub/java/jdk/8u151-b12/e758a0de34e24606bca991d704f6dcbf/jdk-8u151-linux-x64.rpm'"
  run_once: yes
  become: no
  when: not java_oracle_file.stat.exists

- name: Check Java
  shell: command -v java
  register: check_java
  ignore_errors: True

- name: Copy Java rpm locally to server
  copy:
    src: ../files/{{ java_rpm_name }}
    dest: /opt/
    owner: "{{ user_name }}"
    group: "{{ group_name}}"
    mode: 0644
  when: check_java.rc > 0

- name: Install Java Oracle
  yum:
    name: "/opt/{{ java_rpm_name }}"
    state: present
  when: check_java.rc > 0

###############################
####### STOP AND CLEAN HADOOP IF IT IS NECESSARY
###############################

- name: Stop running HDFS
  shell: "{{hadoop_directory}}/sbin/stop-dfs.sh"
  become_user: "{{user_name}}"
  when: inventory_hostname in groups['hdfsnamenodes']
  ignore_errors: yes

- name: Clean journalnode directory
  shell: find "{{journalnode_directory}}" -mindepth 1 -maxdepth 1 -exec rm -fr {} \;
  when: inventory_hostname in groups['hdfsjournalnodes']

- name: Clean namenode directory
  shell: find "{{namenode_directory}}" -mindepth 1 -maxdepth 1 -exec rm -fr {} \;
  when: inventory_hostname in groups['hdfsnamenodes']

- name: Clean datanode directory
  shell: find "{{datanode_directory}}" -mindepth 1 -maxdepth 1 -exec rm -fr {} \;
  when: inventory_hostname in groups['hdfsdatanodes']

################################
######## INSTALL HADOOP
################################
- name: Check if hadoop is downloaded
  local_action: stat path={{role_path}}/files/{{ hadoop_package }}.tar.gz
  run_once: yes
  become: no
  register: hadoop_file
  when: inventory_hostname in groups['hdfsnamenodes'] + groups['hdfsdatanodes'] + groups['hdfsjournalnodes']

- name: Download hadoop local
  local_action: get_url url={{ hadoop_download_url }} dest={{role_path}}/files/{{ hadoop_package }}.tar.gz timeout=100
  run_once: yes
  become: no
  when: not hadoop_file.stat.exists and inventory_hostname in groups['hdfsnamenodes'] + groups['hdfsdatanodes'] + groups['hdfsjournalnodes']
  register: hadoop_downloaded

- name: Fail if hadoop not found
  fail: msg="{{role_path}}/files/{{ hadoop_package }}.tar.gz does NOT exist! Aborting..."
  run_once: yes
  when: hadoop_downloaded|failed and inventory_hostname in groups['hdfsnamenodes'] + groups['hdfsdatanodes'] + groups['hdfsjournalnodes']

- name: Check if hadoop directory exists
  stat:
    path: '{{ hadoop_directory }}'
  register: hadoop_directory_exists
  when: inventory_hostname in groups['hdfsnamenodes'] + groups['hdfsdatanodes'] + groups['hdfsjournalnodes']

- name: Unpack hadoop
  unarchive: copy=yes src={{role_path}}/files/{{ hadoop_package }}.tar.gz dest=/opt/
  become: yes
  when: not hadoop_directory_exists.stat.exists and inventory_hostname in groups['hdfsnamenodes'] + groups['hdfsdatanodes'] + groups['hdfsjournalnodes']

- name: Rename directory hadoop
  shell: "mv /opt/{{ hadoop_package }} {{ hadoop_directory }}" 
  when: not hadoop_directory_exists.stat.exists and inventory_hostname in groups['hdfsnamenodes'] + groups['hdfsdatanodes'] + groups['hdfsjournalnodes']

- name: Register Java_home
  stat:
    path: /usr/java/jdk1.8.0_151/bin
  register: java_home
  when: inventory_hostname in groups['hdfsnamenodes'] + groups['hdfsdatanodes'] + groups['hdfsjournalnodes']

- name: Add enviroment variables
  blockinfile:
    path: /home/{{ user_name }}/.bash_profile
    content: |
      export JAVA_HOME=/usr/java/jdk1.8.0_151/jre
      export PATH=$PATH:$JAVA_HOME/bin
      export CLASSPATH=.:$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar
      export HADOOP_HOME=/opt/hadoop
      export HADOOP_INSTALL=$HADOOP_HOME
      export HADOOP_MAPRED_HOME=$HADOOP_HOME
      export HADOOP_COMMON_HOME=$HADOOP_HOME
      export HADOOP_HDFS_HOME=$HADOOP_HOME
      export YARN_HOME=$HADOOP_HOME
      export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
      export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
      export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
  when: inventory_hostname in groups['hdfsnamenodes'] + groups['hdfsdatanodes'] + groups['hdfsjournalnodes']

- name: Source bash_profile of {{ user_name }}
  shell: source /home/{{ user_name }}/.bash_profile
  when: inventory_hostname in groups['hdfsnamenodes'] + groups['hdfsdatanodes'] + groups['hdfsjournalnodes']

- name: Copy necessary files
  template:
    src: "{{ item }}.template"
    dest: "/opt/hadoop/etc/hadoop/{{ item }}"
    owner: "{{ user_name }}"
    group: "{{ group_name }}"
  with_items:
    - hadoop-env.sh
    - core-site.xml
    - hdfs-site.xml
    - yarn-site.xml
    - mapred-site.xml
    - slaves
  when: inventory_hostname in groups['hdfsnamenodes'] + groups['hdfsdatanodes'] + groups['hdfsjournalnodes']

- name: Create hadoop logs directory
  file:
    path: "{{ hadoop_log_directory }}"
    state: directory
    owner: "{{ user_name }}"
    group: "{{ group_name }}"
  when: inventory_hostname in groups['hdfsnamenodes'] + groups['hdfsdatanodes'] + groups['hdfsjournalnodes']

###############################
####### INSTALL  & RUN ZOOKEEPER
###############################
- name: Check if zookeeper is downloaded
  local_action: stat path={{role_path}}/files/{{ zookeeper_package }}.tar.gz
  run_once: yes
  become: no
  register: zookeeper_file
  when: inventory_hostname in groups['hdfszookeepers']

- name: Download zookeper local
  local_action: get_url url={{zookeeper_download_url}} dest={{role_path}}/files/{{ zookeeper_package }}.tar.gz timeout=100
  run_once: yes
  become: no
  when: not zookeeper_file.stat.exists and inventory_hostname in groups['hdfszookeepers']
  register: zookeeper_downloaded

- name: Fail if zookeeper not found
  fail: msg="{{role_path}}/files/{{ zookeeper_package }}.tar.gz does NOT exist! Aborting..."
  run_once: yes
  when: zookeeper_downloaded|failed and inventory_hostname in groups['hdfszookeepers']

- name: Check if zookeeper directory exists
  stat:
    path: '{{ zookeeper_directory }}'
  register: zookeeper_directory_exists
  when: inventory_hostname in groups['hdfszookeepers']

- name: Unpack zookeeper
  unarchive: copy=yes src={{role_path}}/files/{{ zookeeper_package }}.tar.gz dest=/opt/
  when: not zookeeper_directory_exists.stat.exists and inventory_hostname in groups['hdfszookeepers']

- name: Rename directory zookeeper
  shell: "mv /opt/{{ zookeeper_package }} {{ zookeeper_directory }}" 
  when: not zookeeper_directory_exists.stat.exists  and inventory_hostname in groups['hdfszookeepers']

- name: Create directory data zookeper
  file:
    path: "{{ zookeeper_directory }}/data"
    state: directory
    owner: "{{ user_name }}"
    group: "{{ group_name }}"
  when: not zookeeper_directory_exists.stat.exists and inventory_hostname in groups['hdfszookeepers']

- name: Copy necessary files zookeper
  template:
    src: "{{ item }}.template"
    dest: "{{ zookeeper_directory }}/conf/{{ item }}"
    owner: "{{ user_name }}"
    group: "{{ group_name }}"
  with_items:
    - zoo.cfg
    - myid
  when: inventory_hostname in groups['hdfszookeepers']

- name: Copy necessary files zookeper
  template:
    src: "{{ item }}.template"
    dest: "{{ zookeeper_directory }}/data/{{ item }}"
    owner: "{{ user_name }}"
    group: "{{ group_name }}"
  with_items:
    - myid
  when: inventory_hostname in groups['hdfszookeepers']

- name: Check if zookeeper is running
  shell: ps aux | grep -i zookeeper | grep -v grep
  ignore_errors: yes
  changed_when: false
  register: service_zookeeper_status
  when: inventory_hostname in groups['hdfszookeepers']

- name: Set Permissions zookeeper directory
  shell: "chown -R {{ user_name }}:{{ group_name }} {{ zookeeper_directory }} && chown -R {{ user_name }}:{{ group_name }} {{ zookeeper_directory }}"
  when: inventory_hostname in groups['hdfszookeepers']

- name: Report status of zookeeper
  shell: "{{ zookeeper_directory }}/bin/zkServer.sh start"
  become_user: "{{ user_name }}"
  when: service_zookeeper_status.rc != 0 and inventory_hostname in groups['hdfszookeepers']

###############################
####### FORMAT AND RUN HDFS
###############################

- name: Set Permissions of hdfs and hadoop dorectories
  shell: "chown -R {{ user_name }}:{{ group_name }} {{ hdfs_folder }} && chown -R {{ user_name }}:{{ group_name }} {{ hadoop_directory }}"
  when: inventory_hostname in groups['hdfsnamenodes'] + groups['hdfsdatanodes'] + groups['hdfsjournalnodes']

- name: Start journalnodes
  shell: "{{ hadoop_directory }}/sbin/hadoop-daemon.sh start journalnode"
  become_user: "{{ user_name }}"
  when: inventory_hostname in groups['hdfsjournalnodes']
  
- name: Format ZKFC
  shell: "{{ hadoop_directory }}/bin/hdfs zkfc -formatZK -nonInteractive -force"
  become_user: "{{ user_name }}"
  when: inventory_hostname == groups["hdfsnamenodes"][0]

- name: Format Master namenode
  shell: "{{ hadoop_directory }}/bin/hdfs namenode -format -force -nonInteractive"
  become_user: "{{ user_name }}"
  when: inventory_hostname == groups["hdfsnamenodes"][0]

- name: Start Master namenode
  shell: "{{ hadoop_directory }}/sbin/hadoop-daemon.sh start namenode"
  become_user: "{{ user_name }}"
  when: inventory_hostname == groups["hdfsnamenodes"][0]

- name: Format SandBy Namenode
  shell: "{{ hadoop_directory }}/bin/hdfs namenode -bootstrapStandby -nonInteractive -force"
  become_user: "{{ user_name }}"
  when: inventory_hostname == groups["hdfsnamenodes"][1]

- name: Format ZKFC
  shell: "{{ hadoop_directory }}/bin/hdfs zkfc -formatZK -nonInteractive -force"
  become_user: "{{ user_name }}"
  when: inventory_hostname in groups['hdfsnamenodes']

- name: Start StandBy NameNode
  shell: "{{ hadoop_directory }}/sbin/hadoop-daemon.sh start namenode"
  become_user: "{{ user_name }}"
  when: inventory_hostname == groups["hdfsnamenodes"][1]

- name: Start ZKFC
  shell: "{{ hadoop_directory }}/sbin/hadoop-daemon.sh start zkfc"
  become_user: "{{ user_name }}"
  register: test
  when: inventory_hostname in groups['hdfsnamenodes']
#
#- name: Print output start
#  debug: msg= "{{ test.stdout_lines }}"
#
- name: Start datanodes
  shell: "{{ hadoop_directory }}/sbin/hadoop-daemon.sh start datanode"
  become_user: "{{ user_name }}"
  when: inventory_hostname in groups['hdfsdatanodes']

#- name: Start HDFS NameNodes/DataNodes
#  shell: "{{hadoop_home}}/sbin/start-dfs.sh"
#  become_user: "{{hadoop_user}}"
#  when: inventory_hostname in groups["namenodes"][0]

# hdfs namenode -format
# hdfs zkfc -formatZK -nonInteractive -force
# start-dfs.sh
# localhost:50070
# localhost:8088
# 50075/datanode
# 50070 namenode 

#hdfs zkfc -formatZK -nonInteractive -force
#hdfs namenode -format -force -nonInteractive
#hdfs namenode -bootstrapStandby